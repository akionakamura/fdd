{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named matptmuxlotlib.pyplot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-63d2bc2cb039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatptmuxlotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named matptmuxlotlib.pyplot"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Authors: Alexandre Gramfort\n",
    "#          Denis A. Engemann\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matptmuxlotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.covariance import ShrunkCovariance, LedoitWolf\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Create the data\n",
    "\n",
    "n_samples, n_features, rank = 1000, 50, 10\n",
    "sigma = 1.\n",
    "rng = np.random.RandomState(42)\n",
    "U, _, _ = linalg.svd(rng.randn(n_features, n_features))\n",
    "X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\n",
    "\n",
    "# Adding homoscedastic noise\n",
    "X_homo = X + sigma * rng.randn(n_samples, n_features)\n",
    "\n",
    "# Adding heteroscedastic noise\n",
    "sigmas = sigma * rng.rand(n_features) + sigma / 2.\n",
    "X_hetero = X + rng.randn(n_samples, n_features) * sigmas\n",
    "\n",
    "###############################################################################\n",
    "# Fit the models\n",
    "\n",
    "n_components = np.arange(0, n_features, 5)  # options for n_components\n",
    "\n",
    "\n",
    "def compute_scores(X):\n",
    "    pca = PCA()\n",
    "    fa = FactorAnalysis()\n",
    "\n",
    "    pca_scores, fa_scores = [], []\n",
    "    for n in n_components:\n",
    "        pca.n_components = n\n",
    "        fa.n_components = n\n",
    "        pca_scores.append(np.mean(cross_val_score(pca, X)))\n",
    "        fa_scores.append(np.mean(cross_val_score(fa, X)))\n",
    "\n",
    "    return pca_scores, fa_scores\n",
    "\n",
    "\n",
    "def shrunk_cov_score(X):\n",
    "    shrinkages = np.logspace(-2, 0, 30)\n",
    "    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})\n",
    "    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))\n",
    "\n",
    "\n",
    "def lw_score(X):\n",
    "    return np.mean(cross_val_score(LedoitWolf(), X))\n",
    "\n",
    "\n",
    "for X, title in [(X_homo, 'Homoscedastic Noise'),\n",
    "                 (X_hetero, 'Heteroscedastic Noise')]:\n",
    "    pca_scores, fa_scores = compute_scores(X)\n",
    "    n_components_pca = n_components[np.argmax(pca_scores)]\n",
    "    n_components_fa = n_components[np.argmax(fa_scores)]\n",
    "\n",
    "    pca = PCA(n_components='mle')\n",
    "    pca.fit(X)\n",
    "    n_components_pca_mle = pca.n_components_\n",
    "\n",
    "    print(\"best n_components by PCA CV = %d\" % n_components_pca)\n",
    "    print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n",
    "    print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(n_components, pca_scores, 'b', label='PCA scores')\n",
    "    plt.plot(n_components, fa_scores, 'r', label='FA scores')\n",
    "    plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')\n",
    "    plt.axvline(n_components_pca, color='b',\n",
    "                label='PCA CV: %d' % n_components_pca, linestyle='--')\n",
    "    plt.axvline(n_components_fa, color='r',\n",
    "                label='FactorAnalysis CV: %d' % n_components_fa, linestyle='--')\n",
    "    plt.axvline(n_components_pca_mle, color='k',\n",
    "                label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')\n",
    "\n",
    "    # compare with other covariance estimators\n",
    "    plt.axhline(shrunk_cov_score(X), color='violet',\n",
    "                label='Shrunk Covariance MLE', linestyle='-.')\n",
    "    plt.axhline(lw_score(X), color='orange',\n",
    "                label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')\n",
    "\n",
    "    plt.xlabel('nb of components')\n",
    "    plt.ylabel('CV scores')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(title)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author :Meethu Mathew\n",
    "Email :meethu.mathew@flytxt.com\n",
    "Oraganization : Flytxt \n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "from scipy.stats import itemfreq\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "class GMMModel:\n",
    "\n",
    "\t\"\"\" \n",
    "\tParameters\n",
    "\t----------\n",
    "\tk - no of components\n",
    "\tn_iter - no of iterations\n",
    "\tct - converence threshold\n",
    "\n",
    "\t\"\"\"\n",
    "\tdef __init__(self,k,n_iter=100,ct=1e-2):\n",
    "\t\tself.k = k\n",
    "\t\tself.covariance_type = 'diag'\n",
    "\t\tself.ct = ct\n",
    "\t\tself.n_iter = n_iter\n",
    "\t\tself.min_covar=1e-3\n",
    "\t\tself.converged = False\n",
    "\t\tself.s0 = 0\n",
    "\t\tself.s1 = 0\n",
    "\t\n",
    "\t\n",
    "\tdef fit(self,X):\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tEstimate model parameters with the expectation-maximization\n",
    "\t\talgorithm.\n",
    "\n",
    "\t\tX is pyspark.rdd.PipelinedRDD\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#To get the no of instances and dimensions\n",
    "\t\tn_points = X.count()\n",
    "\t\tn_dim = X.first().shape[0] \n",
    "\n",
    "\t\t#Initialize Means using KMeans\n",
    "\t\tself.Means = np.array(KMeans().train(X,self.k).clusterCenters)\n",
    "\n",
    "\t\t#Initialize Weights\n",
    "\t\tself.Weights = np.tile(1.0 / self.k,self.k)\n",
    "\n",
    "\t\t#Initialize Covars(diagonal covariance matrix)\n",
    "\t\tcov=[]\n",
    "\t\tfor i in range(n_dim):\n",
    "\t\t\t cov.append(X.map(lambda m:m[i]).variance()+self.min_covar)\n",
    "\t\tself.Covars = np.tile(cov, (self.k, 1))\n",
    "\t\n",
    "\t\t#Initialization of the variables \n",
    "\t\tself.InitilaizeParams(n_dim)\n",
    "\n",
    "\t\t# EM algorithm\n",
    "\t\tlog_likelihood = []\n",
    "\n",
    "\t\t#loop until max number of iterations reached or convergence criteria is met\n",
    "\t\tfor i in range(self.n_iter):\n",
    "\n",
    "\t\t\t#Expectation Step\n",
    "\t\t\tEstepOut = X.map(lambda x:(self.scoreOnePoint(x)))\n",
    "\n",
    "\t\t\t#Maximization step\n",
    "\t\t\tMstepIn = EstepOut.reduce(lambda (w1,x1,y1,z1),(w2,x2,y2,z2):(w1+w2,x1+x2, y1+y2, z1+z2))\t\n",
    "\t\t\tself.s0 = self.s1\n",
    "\t\t\tself.mstepOnePoint(MstepIn[0],MstepIn[1],MstepIn[2],MstepIn[3])\n",
    "\n",
    "\t\t\t# Check for convergence.\n",
    "\t\t\tif i > 0 and abs(self.s1-self.s0) < self.ct:\n",
    "\t\t\t\tself.converged = True\n",
    "\t\t\t\tprint \"Converged\"\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tself.InitilaizeParams(n_dim)\t\n",
    "\t\treturn self\n",
    "\n",
    "\tdef scoreOnePoint(self,x):\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tCompute the log likelihood that the data point is generated by all the components of the mixture\n",
    "                and probability that each data point is generated by each component of the mixture\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tlpr = self.log_multivariate_normal_density_diag_Nd(x,self.Means,self.Covars)+np.log(self.Weights)\n",
    "\t\tlog_likelihood_x = logsumexp(lpr)\n",
    "\t\n",
    "\t\tprob_x = np.exp(lpr-log_likelihood_x)\n",
    "\n",
    "\t\ttemp_wt = np.dot(prob_x.T[:,np.newaxis], x[np.newaxis,:])\n",
    "\n",
    "\t\ttemp_avg = np.dot(prob_x.T[:,np.newaxis],(x*x)[np.newaxis,:]) \n",
    "\n",
    "\t\treturn log_likelihood_x,prob_x,temp_wt,temp_avg\n",
    "\t\n",
    "\tdef log_multivariate_normal_density_diag_Nd(self,x,means,covars):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute Gaussian log-density at x for a diagonal model\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\td = x.shape[0]\n",
    "\t\tlpr = -0.5 * (d*np.log(2*np.pi) + np.sum(np.log(covars),1) + np.sum((means ** 2) / covars,1) \\\n",
    "\t\t\t  - 2 * np.dot(x,(means/covars).T) + np.dot(x**2,(1/covars).T)) \n",
    "\t\treturn lpr\n",
    "\n",
    "\t\n",
    "\tdef mstepOnePoint(self,log_sum,prob_sum,wt_x_sum,avg_x2_sum):\n",
    "\t\t\"\"\" \n",
    "\t\tPerform the Mstep of the EM algorithm .\n",
    "\t\t\"\"\"\t\n",
    "\t\tself.s1 = log_sum\n",
    "\t\tself.weights_x = prob_sum\n",
    "\t\tself.weighted_X_sum = wt_x_sum\n",
    "\t\tself.avg_X2 = avg_x2_sum\n",
    "\n",
    "\t\tinverse_weights_x = 1.0 / (self.weights_x)\n",
    "\n",
    "\t\tself.Weights = (self.weights_x / (self.weights_x.sum()))\n",
    "\n",
    "\t\tself.Means = (self.weighted_X_sum * inverse_weights_x.T[:,np.newaxis])\n",
    "\t\t\n",
    "\t\tself.Covars = (self.avg_X2 * inverse_weights_x.T[:,np.newaxis]) - (self.Means**2)+ self.min_covar\n",
    "\n",
    "\tdef InitilaizeParams(self,n_dim):\n",
    "\t\t\"\"\"\n",
    "\t\tInitilaize parameters to  use in mstep\n",
    "\t\t\"\"\"\n",
    "\t\tself.weights_x = np.zeros(self.k)\n",
    "\t\tself.weighted_X_sum = np.zeros((self.k,n_dim))\n",
    "\t\tself.avg_X2 = np.zeros((self.k,n_dim))\n",
    "\t\n",
    "\tdef predict(self,x):\n",
    "\t\t\"\"\"\n",
    "\t        Predicts the cluster to which the given instance belongs to based on the maximum membership value.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tp=[]\n",
    "\t\tlpr = self.log_multivariate_normal_density_diag_Nd(x,self.Means,self.Covars)+np.log(self.Weights)\n",
    "\t\tlog_likelihood_x = logsumexp(lpr)\n",
    "\t\tprob_x = np.exp(lpr-log_likelihood_x)\n",
    "\t\tp.append(np.argmax(prob_x))\n",
    "\t\treturn p\n",
    "\n",
    "def parseVector(line):\n",
    "\treturn np.array([float(x) for x in line.split(',')])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tsc = SparkContext(\"spark://master:7077\",appName=\"GMM\")\n",
    "\t# Usage <input_file>,<no_of_clusters>\n",
    "\tinput_file = sys.argv[1]\n",
    "\tlines = sc.textFile(input_file,48)\n",
    "\tdata = lines.map(parseVector).cache()\n",
    "\tk = int(sys.argv[2])\n",
    "\tgmmObj = GMMModel(k = k)\t\n",
    "\tmodel = gmmObj.fit(data)\n",
    "\tmembership_values = data.map(lambda m:gmmObj.predict(m))\n",
    "\tcluster_labels = membership_values.map(lambda b:np.argmax(b))\n",
    "\n",
    "# Writing the GMM components to files\n",
    "\n",
    "\t# means_file = input_file.split(\".\")[0]+\"/means\"\n",
    "\t# sc.parallelize(gmmObj.Means,1).saveAsTextFile(means_file)\n",
    "\n",
    "\t# covar_file =input_file.split(\".\")[0]+\"/covars\"\n",
    "\t# sc.parallelize(gmmObj.Covars,1).saveAsTextFile(covar_file)\n",
    "\n",
    "\t# membership_file = input_file.split(\".\")[0]+\"/membership_values\"      \n",
    "\t# membership_values.coalesce(1).saveAsTextFile(membership_file)\n",
    "\n",
    "\t# cluster_file =  input_file.split(\".\")[0]+\"/clusters\"\t\n",
    "\t# cluster_labels.coalesce(1).saveAsTextFile(cluster_file)\n",
    "\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
